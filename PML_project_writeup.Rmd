---
title: Practical Machine Learning Project submission
output: html_document
---
#####Executive Summary
This page details the process and code developed to answer the project requirements of Coursera's "Practical Machine Learning" course, part of the Data Science Specialization.  The requirements for the project were to download training and test data from the **Human Activity Recognition** project, and build a model to predict the fashion of exercise performed by participants in the **Weight Lifting Exercises** experiment.  More details on this project and the data can be found here: http://groupware.les.inf.puc-rio.br/har

In order to respond to the Coursera project requirements, I fit two models to the training data: a Partial Least Squares model that performed rather poorly in cross-validation and a Random Forest model that performed with excellent precision.

#####Model Selection
For this project, the task was to consider and evaluate multiple models, using any of the 159 potentially explanatory variables from the HAR datasets, and using cross-validation to generate an expected out of sample error.  For demonstration purposes, I considered a Partial Least Squares (PLS) model and a Random Forest (RF) model using the caret package in R.  After performing some basic data cleanup - removing variables with many or all NAs and those that were not likely to be associated with the performance of the exercise (index numbers, participant's name, etc.), I parsed the training set into a training1 set of 53 variables to tune the models and a validation set that I would use to independently validate the accuracy/OOS errors after using cross-validation to select predictors.  I cleaned the test data using the same procedures, in order to ensure that the predictive model would work properly.

I built a PLS model, using 5-fold Cross-validation across all potential variables.  The PLS model resulted in very poor predictive value - with the best Accuracy potential being 0.3737, giving an Out of Sample Error estimate of `r 1-0.3737`.  This model is just not good for predicting outcomes - output is below.

```{r echo=FALSE}
plsFit<-readRDS("plsFit.rds")
```
```{r}
plsFit
```

Since the PLS model did not seem viable, I next built a Random Forest (RF) model, using the same cross-validation controls as on the PLS model.  In this case, the resulting model, based on CV alone, appears extremely useful with an accuracy values near 1 (first figures below).  When using cross-validation to select model parameters, it is necessary to validate against an independent set of data.  We do this by using the 'predict' function with the rfFit model and the 'validation' dataset that we separated off at the beginning.  Again, examining the confusion matrix of the model's output against the actual values, we see that the RF model has a very high (0.9898) accuracy rate, and associated Out of Sample error estimate (`r 1-0.9898`) (second set of figures below).


#####RF model (figure 1 of 2)
```{r echo=FALSE}
rfFit<-readRDS("rfFit.rds")
cm<-readRDS("rf_cm.rds")
rfFit
```

#####RF model figure 2 of 2
```{r echo=FALSE}
cm
```

As you can see from the rest of the code, I went ahead and tested the results of each model against each other.  This would have been more meaningul had the PLS model shown any promise of positive predictive value.  I will save some space by not displaying the differences generated by ```summary(diff(results)```, but suffice it to say that the difference in accuracy between the two models are statistically significant.

After deciding on the RF model, it is only a matter of predicting on the test data and generating text files for submission to coursera.  This is achieved by the final section of code in the block below.


#####The code that made it all happen:
```{r eval=FALSE}
##get and load the data
library(RCurl)
trainURL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training<-read.csv(trainURL,na.strings=c("NA",""))
test<-read.csv(testURL,na.strings=c("NA",""))

library(caret)


#clean (remove empty variables) and separate out a validation set
summary(training)
training<-training[,7:160]
test<-test[,7:160]
good_data<-apply(!is.na(training),2,sum)>19621
training<-training[,good_data]
test<-test[,good_data]
inTrain<-createDataPartition(y=training$classe,p=0.3,list=FALSE)
training1<-training[inTrain,]
validation<-training[-inTrain,]

#build several models to compare them
#set a trControl for 5-fold cross validation
ctrl <- trainControl(method="cv",number=5)
#build the PLS model
#install.packages('pls')
library(pls)
set.seed(1234)                  
plsFit<-train(classe~.,data=training,method="pls",trControl=ctrl)
plsFit

#build the Random Forest model
ctrl <- trainControl(method="cv",number=5)
rfFit<-train(classe~.,data=training1,method="rf",trControl=ctrl,prox=TRUE,allowParallel=TRUE)
rfFit$finalModel
rfval<-predict(rfFit,newdata=validation)
confusionMatrix(rfval,validation$classe)


#compare results
results<-resamples(list(PLS=plsFit,RF=rfFit))
summary(results)
#get some statistical measures of the differences
summary(diff(results))

#generate predicted values for the test data
tpred<-predict(rfFit,newdata=test)

#generate test results files
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}
pml_write_files(tpred)
```

